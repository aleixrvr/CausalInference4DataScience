---
title: "Propensity Score exercise solution"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r echo=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, comment="", warning = FALSE)
```


```{r libraries}
library(data.table)
library(magrittr)
library(ggplot2)
library(yaml)
library(caret)
library(pROC)
library(glue)
library(DiagrammeR)

set.seed(1234)
```


```{r reading_data}
tuneLength <- 5
confounders <- read_yaml('confounders.yml')
rhc <- fread('rhc_dataset.csv') %>% 
  .[, swang1 := ifelse(swang1=='RHC', 'RHC', 'NRHC')]
confounders %>% 
  paste(collapse=" + ") %>% 
  paste("swang1", ., sep=" ~ ") ->
  formula_
```

```{r death_stats}
rhc[, mean(death == 'Yes'), swang1]
```

## Auxiliary functions

```{r plot_function}
plot_results <- function(ps_values, swang1, name=""){
  auc_res <- auc(roc(swang1, ps_values))
  data.table(
    ps = ps_values, 
    swang1 = swang1
  ) %>% 
    ggplot(aes(ps, group=swang1, fill=swang1, color=swang1)) +
    geom_density(alpha=0.3) +
    ggtitle("AUC {name}: {auc_res %>% round(2)}" %>% glue)
}
```


```{r train_function}
train_model <- function(dt, formula_, k_fold=5, tuneLength=5, category_1, outcome){
  
  # This function will be passed to caret to evaluate the trained models
  auc_metric <- function(data, lev=NULL, model=NULL){
    auc_res <- auc(roc(data$obs, data[[category_1]]))
    return(c(AUC=auc_res))
  }
  
  # We stratify by the outcome to guarantee a balanced dataset with respect the outcome
  index <- createFolds(dt[[outcome]], k_fold, returnTrain = TRUE)
  
  trControl <- caret::trainControl(
    method = "cv", number = k_fold, classProbs = TRUE, 
    summaryFunction=auc_metric, savePredictions='all', index=index)
  
  ps_model <- caret::train(
    formula_ %>% as.formula, data = rhc, method='xgbTree', 
    trControl = trControl, tuneLength = tuneLength, 
    verbose = FALSE, metric = "AUC", maximize=TRUE, 
    objective='binary:logistic', verbosity = 0
  )
  
  return(ps_model)
}
```

## 1. [Propensity scores first attempt]

Train a model for the Propensity Score (PS) with the treatment variable _swang1_.

```{r training_ps, warning=FALSE, message=FALSE}
ps_model_boost <- train_model(rhc, formula_, tuneLength=tuneLength, category_1='RHC', outcome='swang1')
```

These are the best hyperparameters with and its corresponding cross-validated AUC

```{r ps_auc}
print("Best hyperparams: {ps_model_boost$bestTune}" %>% glue)
print("Best AUC: {ps_model_boost$results$AUC %>% max}" %>% glue)
```

*QUESTION*: As you can see, the AUC obtained with boosting is similar to the one obtained with logistic regression. So, why using boosting instead of logistic regression?

We know they have similar AUC a posteriory. In general boosting can fit better to the data (because it provides nonlinear functions). So, if we only use logistic regression, we will not know whether we could have done better fitting using a more complex model.


## 2. [Overfitting]

Use the trained model to make predictions (probabilities) on the same dataset and calculate its AUC. Verify that the AUC with respect to the predicted probabilities is higher than the AUC reported from the cross validation.

```{r ps_overfitting}
ps_values <- predict(
  ps_model_boost, newdata=rhc, type="prob"
)$RHC

plot_results(ps_values, rhc[, swang1], 'boosting') %>% print
```

Notice that the AUC calculated with the predictions `r auc(roc(rhc[, swang1], ps_values))` is quite higher than the reported by the cross validation `r ps_model_boost$results$AUC %>% max`. The latter is a more reliable metric than the former, so we are overfitting our dataset!

*NOTE*: You can try the same process with logistic regression. In that case you will see that, logistic regression overfits less or none at all. So, depending on the model you use, even you use cross validation, you will overfit more or less. 

## 3. [Propensity scores with cross-fitting]

Calculate the PS using 2-fold cross-fitting: split the data set into 2 equally sized data sets $D_1$ and $D_2$. Train a model for PS using $D_1$ and predict on $D_2$, and vice versa. Calculate the AUC with the new propensity score.

```{r}
# we create two datasets D1 and D2 randomly. We stratify by the outcome swang1 to guarantee a balanced dataset with respect the outcome
cross_fit_index <- createDataPartition(rhc$swang1, p = .5, times = 1)

data_1 <- rhc[cross_fit_index$Resample1]
ps_model_boost_1 <- train_model(
  data_1, formula_, tuneLength=tuneLength, category_1='RHC', outcome='swang1'
)

data_2 <- rhc[-cross_fit_index$Resample1]
ps_model_boost_2 <- train_model(
  data_2, formula_, tuneLength=tuneLength, category_1='RHC', outcome='swang1'
)

# We predict on the dataset D1 using the model trained on D2 and vice versa
ps_1 <- predict(ps_model_boost_2, newdata=data_1, type="prob")$RHC
ps_2 <- predict(ps_model_boost_1, newdata=data_2, type="prob")$RHC

rhc %>% 
  .[cross_fit_index$Resample1, ps := ps_1] %>% 
  .[-cross_fit_index$Resample1, ps := ps_2]
```

The obtained AUC with the propensity scores calculated in this way is `r auc(roc(rhc[, swang1], rhc[, ps]))`, which is closer to the one obtained from the cross validation

## 4. [Visual Inspection] 

Make the plot of the density of the PS by treatment group. Are the two groups comparable? 

```{r}
plot_results(rhc[, ps], rhc[, swang1], 'Cross fitted Boosting') %>% print
```

Since the support of both groups is the same, they are fully comparable.

## 5. [ATEs with T-learners and cross-fitting]

Calculate ATEs using T-learner & cross-fitting in order to estimate the effect of swang1 to death:

### Split the data set into 2 equally sized data sets D1 and D2

```{r}
cross_fit_index <- createDataPartition(rhc$swang1, p = .5, times = 1)
data_1 <- rhc[cross_fit_index$Resample1]
data_2 <- rhc[-cross_fit_index$Resample1]
```


### Take $D_1$ and and train two models:

- With swang1 = RHC, called $f_{1,R}$
- With swang1 = Non-RHC, called $f_{1,N}$

```{r}
formula_ <- 'death ~ ps'

data_1_rhc <- data_1[swang1=='RHC']
ps_model_boost_1_rhc <- train_model(
  data_1_rhc[, .(death, ps)], formula_,
  tuneLength=tuneLength, 
  category_1='Yes', outcome='death'
)

data_1_nrhc <- data_1[swang1=='NRHC']
ps_model_boost_1_nrhc <- train_model(
  data_1_nrhc[, .(death, ps)], formula_,
  tuneLength=tuneLength, 
  category_1='Yes', outcome='death'
)
```

### Repeat the process with $D_2$ and train two models


```{r}
data_2_rhc <- data_2[swang1=='RHC']
ps_model_boost_2_rhc <- train_model(
  data_2_rhc[, .(death, ps)], formula_,
  tuneLength=tuneLength, 
  category_1='Yes', outcome='death'
)

data_2_nrhc <- data_2[swang1=='NRHC']
ps_model_boost_2_nrhc <- train_model(
  data_2_nrhc[, .(death, ps)], formula_,
  tuneLength=tuneLength, 
  category_1='Yes', outcome='death'
)
```

### Calculate on $D_2$ the vector of predictions $f_{1,R}(x) - f_{1,N}(x)$ where $x$ ranges for all observations in D2.

Later, switch roles between D1 and D2 and calculate the ATE.

```{r}
ATE_1 <- predict(ps_model_boost_2_rhc, newdata=data_1, type="prob")$Yes
ATE_1 <- ATE_1 - predict(ps_model_boost_2_nrhc, newdata=data_1, type="prob")$Yes
ATE_2 <- predict(ps_model_boost_1_rhc, newdata=data_2, type="prob")$Yes
ATE_2 <- ATE_2 - predict(ps_model_boost_1_nrhc, newdata=data_2, type="prob")$Yes
```

The ATE is
```{r}
print( mean(c(ATE_1, ATE_2)))
```

